{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Web Scrape [DataJobs.com](https://datajobs.com/)\n",
    "\n",
    "If you're like me, endlessly scrolling through job postings to gain insights on what you should learn ends up becoming an excersize in futility. I need a way to digest all of the information from thousands of job postings all at once. That is what this project aims to do: scrape information from online job boards to gain valauable job-hunting insights!\n",
    "\n",
    "<img src=\"IMG/DataJobs_Header.png\">\n",
    "\n",
    "This notebook will demonstrate how to scrape job entries from [DataJobs.com](https://datajobs.com/) using [Selenium](https://selenium-python.readthedocs.io/)! This serves as a guide informing the larger scraper that incorporates jobs from [Indeed.com](https://www.indeed.com/). Please see the [Webscrape_Datajobs](https://github.com/ColinB19/datajobswebscraper/blob/master/Webscrape_DataJobs.ipynb) notebook for the full end-product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enables automatic installation of chrome drivers\n",
    "service = Service(ChromeDriverManager().install())\n",
    "# set up chrome driver\n",
    "driver = Chrome(service=service)\n",
    "\n",
    "# navigate to DataJobs.com\n",
    "site_url = \"https://datajob.com/\"\n",
    "driver.get(site_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have navigated to the site page, let's click on the *Data Science Jobs/Analytics' link to get a list of all data science rolls available on the platform.\n",
    "\n",
    "<img src=\"IMG/DataJobs_link1-edit.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_time = 3\n",
    "dsa_jobs_list = WebDriverWait(driver, wait_time).until(\n",
    "    EC.element_to_be_clickable(\n",
    "        (By.XPATH, \"//a[contains(text(), 'Data Science Jobs / Analytics')]\")\n",
    "    )\n",
    ")\n",
    "dsa_jobs_list.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can scroll through the jobs and pull information. The scraper should grab all of the entries on a page, then go to the next page and repeat the process. What will we scrape?\n",
    "\n",
    "1. The link to the job posting - so we can scrape job descriptions later!\n",
    "2. The Job Title\n",
    "3. The Company\n",
    "4. Pay information (if available)\n",
    "5. The Location\n",
    "\n",
    "<img src=\"IMG/scroll-edit.gif\" height=400 width=600>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the regex we will use to parse the HTML\n",
    "dj_pattern = r\"<a href=\\\"(.*)\\\"><strong>(.*)</strong> – <span [^\\>]*>(.*)</span></a>[\\n\\s]*</div>[\\n\\s]*<div[^\\>]*>[\\n\\s]*<em>[\\n\\s]*<span[^\\>]*>(.*)</span>[\\n\\s]*[\\&nbsp;\\•]*[\\n\\s]*\\$*([\\d,]*)[–\\s]*\\$*([\\d,]*)[\\n\\s]*</em>\"\n",
    "\n",
    "board_paths = [\"/Data-Science-Jobs\", \"/Data-Engineering-Jobs\"]\n",
    "# loop through the boards available\n",
    "for bp in board_paths:\n",
    "    if bp == \"/Data-Science-Jobs\":\n",
    "        cat = \"Data Science & Analytics\"\n",
    "    else:\n",
    "        cat = \"Data Engineering\"\n",
    "    # load into the webpage\n",
    "    driver.get(site_url + bp)\n",
    "    more_pages = True  # will kill the loop when there are no more pages\n",
    "    i = 0  # just a counter to kill the loop just in case\n",
    "    while more_pages:\n",
    "        # grab page source html\n",
    "        page_html = driver.page_source\n",
    "\n",
    "        # grab job info\n",
    "        fall = re.findall(dj_pattern, page_html)\n",
    "        \n",
    "        # zip the info into a dict for easy DataFrame-ability\n",
    "        fall_cols = [\n",
    "            dict(\n",
    "                zip(\n",
    "                    job_meta.columns,\n",
    "                    (\n",
    "                        (\n",
    "                            y.replace(\"&amp;\", \"&\") # this removes some HTML stuff to not confuse the CSV format\n",
    "                            .replace(\"&amp,\", \"&\")\n",
    "                            .replace(\"&nbsp;\", \" \")\n",
    "                            .replace(\"&nbsp,\", \" \")\n",
    "                            if type(y) == str\n",
    "                            else y\n",
    "                        )\n",
    "                        for y in x\n",
    "                    )\n",
    "                    + (cat,),\n",
    "                )\n",
    "            )\n",
    "            for x in fall\n",
    "        ]\n",
    "        # add to dataframe\n",
    "        job_meta = pd.concat(\n",
    "            [job_meta, pd.DataFrame(fall_cols)], ignore_index=True\n",
    "        )\n",
    "\n",
    "        if i == 300:\n",
    "            # stop after 300 pages\n",
    "            more_pages = False\n",
    "\n",
    "        # try to go to next page\n",
    "        try:\n",
    "            next_page = WebDriverWait(driver, wait_time).until(\n",
    "                EC.element_to_be_clickable(\n",
    "                    (By.XPATH, \"//a[contains(text(), 'NEXT PAGE')]\")\n",
    "                )\n",
    "            )\n",
    "            next_page.click()\n",
    "            i += 1\n",
    "        except:\n",
    "            print(f\"END OF SEARCH RESULTS: {site_url} || {bp}\")\n",
    "            more_pages = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's scrape job descriptions. This is so we can find some popular skills and terms for data jobs.\n",
    "\n",
    "<img src=\"IMG/JobPost_HTML_example.png\">\n",
    "\n",
    "First, we can utilize the links we scraped earlier to navigate to the job posting webpage:\n",
    "\n",
    "```python\n",
    "# string subsetting to have proper nmber of '/' characters in the link\n",
    "job_url = site_url + job[\"url\"][1:]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(html_string: str) -> str:\n",
    "    \"\"\"Regex pattern to match and remove all html tags and comments, leaving plain text.\"\"\"\n",
    "    html_string2 = re.sub(\"(<!--.*?-->)\", \"\", html_string, flags=re.DOTALL)\n",
    "    cleaned_html = re.sub(\n",
    "        \"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\", \" \", html_string2\n",
    "    )\n",
    "    return cleaned_html\n",
    "\n",
    "# list to hold description text\n",
    "job_desc_list = []\n",
    "for _, job in job_meta.iterrows():\n",
    "    # set up the URL so the driver can navigate there\n",
    "    job_url = site_url + job[\"url\"][1:]\n",
    "    # navigate to the job posting\n",
    "    driver.get(job_url)\n",
    "    # grab job desc element\n",
    "    try:\n",
    "        job_descr = WebDriverWait(driver, wait_time).until(\n",
    "            EC.element_to_be_clickable(\n",
    "                (\n",
    "                    By.XPATH,\n",
    "                    \"//div[@id='job_description']//*[@class='jobpost-table-cell-2']\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    except:\n",
    "        print(f\"I can't find this job: {job['title']} || {self._site_url}\")\n",
    "        continue\n",
    "\n",
    "    # get html\n",
    "    job_desc_clean = (\n",
    "        cleanhtml(job_descr.get_attribute(\"innerHTML\"))\n",
    "        .replace(\"&amp;\", \"&\") # this removes some HTML stuff to not confuse the CSV format\n",
    "        .replace(\"&amp,\", \"&\")\n",
    "        .replace(\"&nbsp;\", \" \")\n",
    "        .replace(\"&nbsp,\", \" \")\n",
    "    )\n",
    "    job_desc_list.append(\n",
    "        {\n",
    "            \"job_id\": job[\"job_id\"],\n",
    "            \"title\": job[\"title\"],\n",
    "            \"company\": job[\"company\"],\n",
    "            \"desc\": job_desc_clean,\n",
    "        }\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wellfound",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
